{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86edf95c",
   "metadata": {},
   "source": [
    "# Working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91e993f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib\n",
    "from scipy import sparse\n",
    "from implicit.als import AlternatingLeastSquares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load news data\n",
    "news_data = pd.read_csv(r\"C:\\Users\\HP\\Desktop\\news.tsv\", sep='\\t', header=None)\n",
    "\n",
    "# Combine title and description into a single text field\n",
    "news_data['combined_text'] = news_data[3] + \" \" + news_data[4]\n",
    "\n",
    "# Drop rows with missing values in the combined text field\n",
    "news_data_cleaned = news_data.dropna(subset=['combined_text'])\n",
    "\n",
    "# Vectorize the combined text field using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_matrix = vectorizer.fit_transform(news_data_cleaned['combined_text'])\n",
    "\n",
    "# Save the vectorizer and TF-IDF matrix for future use\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')\n",
    "joblib.dump(tfidf_matrix, 'tfidf_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78374c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cosine_sim_matrix.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Save the cosine similarity matrix\n",
    "joblib.dump(cosine_sim_matrix, 'cosine_sim_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26561b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user behavior data\n",
    "behaviors = pd.read_csv(r\"C:\\Users\\HP\\Desktop\\behaviors.tsv\", sep='\\t', header=None, names=['ImpressionID', 'UserID', 'Time', 'History', 'Impressions'])\n",
    "\n",
    "# Extract user interactions (impressions with clicks)\n",
    "data = []\n",
    "for index, row in behaviors.iterrows():\n",
    "    user_id = row['UserID']\n",
    "    impressions = row['Impressions'].split()  # List of news items and click flags\n",
    "    for impression in impressions:\n",
    "        news_id, click_flag = impression.split('-')  # Split on the \"-\"\n",
    "        data.append((user_id, news_id, int(click_flag)))  # Store as (user_id, news_id, click_flag)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "interaction_df = pd.DataFrame(data, columns=['user_id', 'news_id', 'click_flag'])\n",
    "\n",
    "# Encode user_id and news_id as categorical values\n",
    "interaction_df['user_id'] = interaction_df['user_id'].astype('category')\n",
    "interaction_df['news_id'] = interaction_df['news_id'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b1754c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended articles for user U13740: ['N43787' 'N27101' 'N36836' 'N25091' 'N43587' 'N34544' 'N52294' 'N560'\n",
      " 'N20147' 'N26767']\n"
     ]
    }
   ],
   "source": [
    "def get_content_recommendations(user_id, interaction_df, news_data_cleaned, cosine_sim_matrix, N=10):\n",
    "    \"\"\"\n",
    "    Recommend N articles for a user based on content similarity.\n",
    "    Args:\n",
    "        user_id: The target user's ID.\n",
    "        interaction_df: DataFrame containing user interactions with articles.\n",
    "        news_data_cleaned: DataFrame containing cleaned news articles.\n",
    "        cosine_sim_matrix: Precomputed cosine similarity matrix.\n",
    "        N: Number of recommendations to return.\n",
    "    \n",
    "    Returns:\n",
    "        List of recommended article IDs.\n",
    "    \"\"\"\n",
    "    # Get all news articles the user has interacted with\n",
    "    user_history = interaction_df[interaction_df['user_id'] == user_id]\n",
    "    \n",
    "    # Filter out news articles that are not present in the cleaned news data\n",
    "    valid_news_ids = news_data_cleaned[0].values  # News article IDs from news_data_cleaned\n",
    "    user_history_filtered = user_history[user_history['news_id'].isin(valid_news_ids)]\n",
    "    \n",
    "    if user_history_filtered.empty:\n",
    "        return f\"No valid articles found for user {user_id}.\"\n",
    "    \n",
    "    # Create a mapping of news_id to its index in news_data_cleaned\n",
    "    news_id_to_index = {news_id: idx for idx, news_id in enumerate(news_data_cleaned[0])}\n",
    "    \n",
    "    # Get the indices of these articles in the cleaned news_data\n",
    "    user_article_indices = [news_id_to_index[news_id] for news_id in user_history_filtered['news_id']]\n",
    "\n",
    "    # Compute the average similarity score for all articles based on the user's clicked articles\n",
    "    sim_scores = cosine_sim_matrix[user_article_indices].mean(axis=0)\n",
    "    \n",
    "    # Get the indices of the top N most similar articles\n",
    "    top_article_indices = sim_scores.argsort()[-N:][::-1]\n",
    "    \n",
    "    # Get the corresponding article IDs for the top recommendations\n",
    "    recommended_articles = news_data_cleaned.iloc[top_article_indices][0].values\n",
    "    \n",
    "    return recommended_articles\n",
    "\n",
    "# Example user ID\n",
    "user_id = 'U13740'\n",
    "\n",
    "# Get recommendations for the user\n",
    "recommended_articles = get_content_recommendations(user_id, interaction_df, news_data_cleaned, cosine_sim_matrix, N=10)\n",
    "\n",
    "print(f\"Recommended articles for user {user_id}: {recommended_articles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f9fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82f11083",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = pd.read_csv(\n",
    "    r\"C:\\Users\\HP\\Desktop\\Flask-News-app\\data\\behaviors.tsv\",\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['ImpressionID', 'UserID', 'Time', 'History', 'Impressions']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33687133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "241747fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for index, row in behaviors.iterrows():\n",
    "    user_id = row['UserID']\n",
    "    impressions = row['Impressions'].split()  # List of news items and click flags\n",
    "    for impression in impressions:\n",
    "        if '-' in impression:\n",
    "            news_id, click_flag = impression.split('-')  # Split on the \"-\"\n",
    "            try:\n",
    "                click_flag = int(click_flag)\n",
    "                data.append((user_id, news_id, click_flag))  # Store as (user_id, news_id, click_flag)\n",
    "            except ValueError:\n",
    "                # Handle cases where click_flag is not an integer\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3747ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_df = pd.DataFrame(data, columns=['user_id', 'news_id', 'click_flag'])\n",
    "\n",
    "# Filter out interactions where click_flag is not positive (assuming click_flag=1 indicates a click)\n",
    "interaction_df = interaction_df[interaction_df['click_flag'] > 0].reset_index(drop=True)\n",
    "\n",
    "# Encode user_id and news_id as categorical values\n",
    "interaction_df['user_id'] = interaction_df['user_id'].astype('category')\n",
    "interaction_df['news_id'] = interaction_df['news_id'].astype('category')\n",
    "\n",
    "# Create mappings for user_id and news_id\n",
    "user_id_mapping = {cat: code for code, cat in enumerate(interaction_df['user_id'].cat.categories)}\n",
    "news_id_mapping = {cat: code for code, cat in enumerate(interaction_df['news_id'].cat.categories)}\n",
    "reverse_news_id_mapping = {code: cat for cat, code in news_id_mapping.items()}\n",
    "\n",
    "# Encode user_id and news_id\n",
    "interaction_df['user_id_encoded'] = interaction_df['user_id'].cat.codes\n",
    "interaction_df['news_id_encoded'] = interaction_df['news_id'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96787d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\Flask-News-app\\venv\\Lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: OpenBLAS is configured to use 12 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.43it/s]\n"
     ]
    }
   ],
   "source": [
    "user_item_matrix = sparse.csr_matrix(\n",
    "    (interaction_df['click_flag'], \n",
    "     (interaction_df['user_id_encoded'], interaction_df['news_id_encoded']))\n",
    ")\n",
    "joblib.dump(user_item_matrix, 'models/user_item_matrix.pkl')\n",
    "# Initialize and train ALS model\n",
    "als_model = AlternatingLeastSquares(factors=50, regularization=0.1, iterations=20)\n",
    "als_model.fit(user_item_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed1cf087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collaborative_recommendations(user_id, interaction_df, als_model, user_id_mapping, reverse_news_id_mapping, user_item_matrix, N=10):\n",
    "    \"\"\"\n",
    "    Recommend N articles for a user based on collaborative filtering (ALS).\n",
    "    \"\"\"\n",
    "    if user_id not in user_id_mapping:\n",
    "        return f\"UserID {user_id} not found in interaction data.\"\n",
    "\n",
    "    # Get the encoded user ID\n",
    "    user_id_encoded = user_id_mapping[user_id]\n",
    "    \n",
    "    # Check if the user has interacted with any items\n",
    "    if user_id_encoded >= user_item_matrix.shape[0]:\n",
    "        return f\"UserID {user_id} has no interactions.\"\n",
    "\n",
    "    # Get the top N recommendations for the user\n",
    "    recommended_articles_encoded, _ = als_model.recommend(user_id_encoded, user_item_matrix[user_id_encoded], N=N, filter_already_liked_items=True)\n",
    "    \n",
    "    # Map back to original news IDs\n",
    "    recommended_articles = [reverse_news_id_mapping[news_id] for news_id in recommended_articles_encoded]\n",
    "    \n",
    "    return recommended_articles\n",
    "\n",
    "# Content-Based Recommendation Function\n",
    "def get_content_recommendations(user_id, interaction_df, news_data_cleaned, cosine_sim_matrix, news_id_mapping, N=10):\n",
    "    \"\"\"\n",
    "    Recommend N articles for a user based on content similarity.\n",
    "    \"\"\"\n",
    "    if user_id not in interaction_df['user_id'].cat.categories:\n",
    "        return f\"UserID {user_id} not found in interaction data.\"\n",
    "    \n",
    "    # Get all news articles the user has interacted with\n",
    "    user_history = interaction_df[interaction_df['user_id'] == user_id]\n",
    "    \n",
    "    # Filter out news articles that are not present in the cleaned news data\n",
    "    valid_news_ids = set(news_data_cleaned[0].astype(str))\n",
    "    user_history_filtered = user_history[user_history['news_id'].isin(valid_news_ids)]\n",
    "    \n",
    "    if user_history_filtered.empty:\n",
    "        return f\"No valid articles found for user {user_id}.\"\n",
    "    \n",
    "    # Create a mapping of news_id to its index in news_data_cleaned\n",
    "    news_id_to_index = {news_id: idx for idx, news_id in enumerate(news_data_cleaned[0].astype(str))}\n",
    "    \n",
    "    # Get the indices of these articles in the cleaned news_data\n",
    "    user_article_indices = [news_id_to_index[news_id] for news_id in user_history_filtered['news_id']]\n",
    "    \n",
    "    # Compute the average similarity score for all articles based on the user's clicked articles\n",
    "    sim_scores = cosine_sim_matrix[user_article_indices].mean(axis=0)\n",
    "    \n",
    "    # Get the indices of the top N most similar articles\n",
    "    top_article_indices = sim_scores.argsort()[-N:][::-1]\n",
    "    \n",
    "    # Get the corresponding article IDs for the top recommendations\n",
    "    recommended_articles = news_data_cleaned.iloc[top_article_indices][0].astype(str).tolist()\n",
    "    \n",
    "    return recommended_articles\n",
    "\n",
    "# Hybrid Recommendation System\n",
    "def get_hybrid_recommendations(user_id, interaction_df, news_data_cleaned, cosine_sim_matrix, \n",
    "                               als_model, user_id_mapping, reverse_news_id_mapping, user_item_matrix, \n",
    "                               N=10, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Recommend N articles for a user using a hybrid approach (blending content-based and collaborative filtering).\n",
    "    \n",
    "    Args:\n",
    "    - user_id: The target user ID.\n",
    "    - interaction_df: DataFrame containing user interactions.\n",
    "    - news_data_cleaned: DataFrame containing cleaned news articles.\n",
    "    - cosine_sim_matrix: Precomputed cosine similarity matrix for content-based filtering.\n",
    "    - als_model: Trained ALS collaborative filtering model.\n",
    "    - user_id_mapping: Dictionary mapping user IDs to encoded IDs.\n",
    "    - reverse_news_id_mapping: Dictionary mapping encoded news IDs to original news IDs.\n",
    "    - user_item_matrix: Sparse matrix used by ALS.\n",
    "    - N: Number of recommendations to return.\n",
    "    - alpha: Weight factor to blend collaborative and content-based scores (0 <= alpha <= 1).\n",
    "    \n",
    "    Returns:\n",
    "    - List of recommended article IDs.\n",
    "    \"\"\"\n",
    "    # Get collaborative recommendations\n",
    "    collaborative_recommendations = get_collaborative_recommendations(\n",
    "        user_id, interaction_df, als_model, user_id_mapping, reverse_news_id_mapping, user_item_matrix, N\n",
    "    )\n",
    "    \n",
    "    # Get content-based recommendations\n",
    "    content_recommendations = get_content_recommendations(\n",
    "        user_id, interaction_df, news_data_cleaned, cosine_sim_matrix, news_id_mapping, N\n",
    "    )\n",
    "    \n",
    "    # Handle cases where recommendations might be strings (error messages)\n",
    "    if isinstance(collaborative_recommendations, str):\n",
    "        collaborative_recommendations = []\n",
    "    if isinstance(content_recommendations, str):\n",
    "        content_recommendations = []\n",
    "    \n",
    "    # Convert all recommendations to lists if they aren't already\n",
    "    collaborative_recommendations = list(collaborative_recommendations)\n",
    "    content_recommendations = list(content_recommendations)\n",
    "    \n",
    "    # Determine the number of recommendations to take from each method\n",
    "    num_collaborative = int(N * alpha)\n",
    "    num_content = N - num_collaborative\n",
    "    \n",
    "    # Slice the recommendation lists\n",
    "    collaborative_slice = collaborative_recommendations[:num_collaborative]\n",
    "    content_slice = content_recommendations[:num_content]\n",
    "    \n",
    "    # Combine the two sets of recommendations, ensuring no duplicates\n",
    "    combined_recommendations = collaborative_slice + [item for item in content_slice if item not in collaborative_slice]\n",
    "    \n",
    "    # If combined_recommendations are fewer than N, fill the remaining with other recommendations\n",
    "    if len(combined_recommendations) < N:\n",
    "        additional_needed = N - len(combined_recommendations)\n",
    "        # Combine all unique recommendations from both methods\n",
    "        all_unique = list(set(collaborative_recommendations + content_recommendations))\n",
    "        # Exclude already recommended items\n",
    "        additional_recommendations = [item for item in all_unique if item not in combined_recommendations]\n",
    "        # Add the additional recommendations\n",
    "        combined_recommendations += additional_recommendations[:additional_needed]\n",
    "    \n",
    "    # Return the top N recommendations\n",
    "    return combined_recommendations[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "340677e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid recommendations for user U13740: ['N52622', 'N59981', 'N55204', 'N56193', 'N31947', 'N21707', 'N35937', 'N55689', 'N58133', 'N28910']\n"
     ]
    }
   ],
   "source": [
    "user_id = 'U13740'\n",
    "\n",
    "# Get hybrid recommendations for the user\n",
    "hybrid_recommendations = get_hybrid_recommendations(\n",
    "    user_id, \n",
    "    interaction_df, \n",
    "    news_data_cleaned, \n",
    "    cosine_sim_matrix, \n",
    "    als_model, \n",
    "    user_id_mapping, \n",
    "    reverse_news_id_mapping, \n",
    "    user_item_matrix, \n",
    "    N=10, \n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Display recommendations\n",
    "print(f\"Hybrid recommendations for user {user_id}: {hybrid_recommendations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b5259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd9e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38137d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae7d4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b113ba76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d690fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d8002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f6375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07f2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3244322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230ae2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b5bac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269499ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
